{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mu68fLX8Itm9"
      },
      "source": [
        "# Sieci rekurencyjne (RNN) + Keras\n",
        "\n",
        "Klasyczne sieci neuronowe mają dwie zasadnicze wady, które sprawiają, że przetwarzanie języka naturalnego z ich udziałem jest problematyczne:\n",
        "<ul>\n",
        "    <li>Wyuczone wagi powiązane są z konkretnymi pozycjami cechy w wektorze cech</li>\n",
        "    <li>Ciężko uwzględnić relacje między cechami (n-gramy pomagają tylko trochę)</li>\n",
        "</ul>\n",
        "\n",
        "Kiedy rozważamy problemy przetwarzania języka - cechami są słowa w zdaniach, bądź jakieś statystyki powiązane ze słowami. Gramatyka większości języków (w szczególności fleksyjnych, takich jak nasz) pozwala jednak na pojawienie się istotnych wyrażeń w różnych miejscach w zdaniu. Kiedy chcielibyśmy wykonać zadanie wykrywania nazw firm w zdaniach - pozycja słowa w zdaniu dostarcza niewielu informacji o tym czy dane słowo jest rzeczywiście firmą czy nie.\n",
        "\n",
        "**Apple** jest najlepszą firmą.\n",
        "\n",
        "Najlepszą firmą jest **Apple**.\n",
        "\n",
        "Najlepsza firma - to **Apple**.\n",
        "\n",
        "Firma **Apple** jest najlepsza.\n",
        "\n",
        "Co więcej - często wcześniejsze słowa mają wpływ na kolejne. Wyobraźmy sobie zadanie uzupełniania luki w zdaniu. \n",
        "Mamy dwa zdania:\n",
        "\n",
        "\n",
        "W pracy zawodowej piszą bardzo dużo kodu, jestem [   ].\n",
        "\n",
        "W pracy zawodowej często pomagam ludziom, jestem [   ].\n",
        "\n",
        "Słowa poprzedzające luki pozwalają nam z dużo większym prawdopodobieństwem oszacować, że w pierwszym przypadku powinniśmy umieścić zawód programisty lub podobny, a w drugim - pielęgniarki lub podobny. Zatem poprzednie elementy sekwencji, mają wpływ na kolejne.\n",
        "\n",
        "Pomóc w takich problemach mogą sieci rekurencyjne.\n",
        "\n",
        "Dzisiejsze laboratoria pokażą jak zaimplementować sieć rekurencyjną od zera i jak nauczyć ją dodawania do siebie dwóch liczb reprezentowanych w postaci stringów (w formacie binarnym - jako ciągi zer i jedynek). (Np. 7=\"111\", 5=\"101\"). \n",
        "\n",
        "\n",
        "Dlaczego taki problem? Dodawanie dwóch reprezentacji binarnych jest bardzo prostym zadaniem, w którym sekwencja znaków jest istotna. Kiedy dodajemy dwie liczby binarne, często mamy do czynienia z potrzebą uwzględnienia bitu przeniesienia z aktualnego do kolejnego kroku (kiedy dodajemy do siebie 1 + 1) - nasza sieć neuronowa nauczy się uwzględniać informację o tym, ucząc się wykorzystania pamięci znajdującej się w warstwie ukrytej.\n",
        "\n",
        "![unfolded_rnn.png](attachment:unfolded_rnn.png)\n",
        "\n",
        "Powyższy obrazek pokazuje sieć rekurencyjną. Widzimy, że zawiera ona w sobie klasyczną sieć feedforward składającą się z 1 warstwy ukrytej (hidden layer) oraz następującej po niej warstwie wyjściowej (output layer). Aby przetworzyć sekwencję, ta sama sieć jest zwielokrotniona (w poziomie) i przetwarza po kolei każdy element z sekwencji. Na powyższym obrazku sieć feedforward (na czarno) ma 3 kopie - przetwarza więc sekwencję 3-elementową (np. 3 następujące po sobie słowa).\n",
        "\n",
        "Przekazywanie informacji o poprzednich elementach sekwencji (np. słowach) do kolejnych następuje poprzez połączenie warstwy ukrytej w kolejnych krokach czasowych (czerwone połączenia) Warstwa ukryta powinna modelować pamięć, która może być użyta do podejmowania przyszłych decyzji. \n",
        "\n",
        "Literki U, V, W symbolizują macierze wag pomiędzy warstwami. U łączy warstwę wejściową z ukrytą (każdy z każdym), V - łączy ukrytą z wyjściową (każdy z każdym), a W łączy warstwę ukrytą z poprzedniego punktu w czasie z kolejnym (również każdy z każdym).\n",
        "\n",
        "Warto zauważyć, że do przetworzenia każdego kroku sekwencji używamy dokładnie tych samych wag. Wagi U, V, W są użyte przy każdej \"kopii\" sieci rekurencyjnej (jak na obrazku). \n",
        "\n",
        "---\n",
        "\n",
        "![layers.jpg](attachment:layers.jpg)\n",
        "\n",
        "Sieci rekurencyjne stwarzają nam kilka możliwości tworzenia architektur. Powyżej zwizualizowanych jest kilka modeli z jedną warstwą ukrytą.\n",
        "\n",
        "Dane w sekwencji następują po sobie jeden po drugim (słowa tworzą uporządkowany ciąg), każdy z elementów ciągu nadchodzi później, reprezentuje więc późniejszy punkt w czasie. Oś pozioma (od lewej do prawej) na powyższym obrazku symbolizuje upływ czasu - kolejne kroki wynikające z przyjmowania/generowania kolejnych elementów sekwencji.\n",
        "\n",
        "Różowy prostokąt - wektor cech warstwy wejściowej, dla aktualnego punktu w czasie\n",
        "\n",
        "Zielony prostokąt - wektor warstwy ukrytej dla aktualnego punktu w czasie\n",
        "\n",
        "Niebieski prostokąt - wektor warstwy wyjściowej dla aktualnego punktu w czasie\n",
        "\n",
        "\n",
        "<ol>\n",
        "<li>one to one - klasyczna sieć feedforward, wektor cech transformowany jest do warstwy ukrytej, ta zaś transformowana jest do warstwy wyjśiowej - brak rekurencji, brak sekwencji</li>\n",
        "<li>one to many - tworzenie sekwencji z pojedynczego wektora cech. Np. tworzenie opisu obrazków mając zadany 1 obrazek (opis - sekwencja wielu wyrazów, wejście 1 obrazek)</li>\n",
        "<li>many to one - tworzenie pojedynczego wyjścia dla sekwencji wielu wejść. Np. Wykrywanie sentymentu w recenzji tekstowej. Z sekwencji wielu wejść (wiele słów recenzji) generujemy pojedynczą decyzję (sentyment powiązany z recenzją).</li>\n",
        "<li>many to many - Z sekwencji N wejść, tworzymy sekwencję M wyjść. Np. tłumaczenie maszynowe - na wejściu sekwencja słów w jednym języku, na wyjściu zaś przetłumaczona sekwencja słów (potencjalnie o innej długości!)</li>\n",
        "<li>many to many (synchronizowane) - generowanie sekwencji o długości takiej jak sekwencja wejściowa. Np. klasyfikacja kolejnych klatek wideo.</li>\n",
        "</ol>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XGY-eLhaItnD"
      },
      "source": [
        "## Materiały online:\n",
        "**Ponieważ, podobnie jak w przypadku poprzednich laboratoriów, najłatwiej byłoby pokazać działanie sieci rekurencyjnych przy tablicy, a trochę ciężej zrobić to za pomocą statycznych obrazków, polecam poniższy materiał na YouTube, który w intuicyjny sposób przedstawia ideę sieci rekurencyjnych i problemy z ich wykorzystaniem (https://www.youtube.com/watch?v=LHXXI4-IEns - długość ok. 10 minut)**\n",
        "\n",
        "## Zadanie1 (0.5 pkt): Inicjalizacja wag sieci\n",
        "\n",
        "Wagi na połączeniach między warstwami powinny być początkowo niewielkimi losowymi odchyleniami od wartości 0. W procesie uczenia wartości te modyfikować się będą tak, aby jak najdokładniej przewidywać oczekiwane wyjście.\n",
        "\n",
        "**Wykorzystując numpy, napisz funkcję, która wygeneruje losową macierz zadanych rozmiarów (rows - liczba wierszy, cols - liczba kolumn), której wartości będą zawarte w przedziale -1 do 1.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sIkbNOvVItnE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.09762701  0.43037873  0.20552675]\n",
            " [ 0.08976637 -0.1526904   0.29178823]\n",
            " [-0.12482558  0.783546    0.92732552]\n",
            " [-0.23311696  0.58345008  0.05778984]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "def generate_random_matrix(rows, cols):\n",
        "    return np.random.uniform(-1,1,(rows,cols))\n",
        "\n",
        "print(generate_random_matrix(4, 3))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BPyxY8-AItnF"
      },
      "source": [
        "## Zadanie 2 (1 pkt): Wyznaczenie aktywacji warstwy ukrytej (pamięci) i wyjściowej\n",
        "\n",
        "W przypadku sieci rekurencyjnych, w przeciwieństwie do klasycznych feedforward - pojawia się połączenie rekursywne z historycznymi danymi. \n",
        "Jeśli stworzymy prostą rekurencyjną sieć neuronową z jedną warstwą ukrytą, to obliczenia w tej sieci wyglądać będą następująco:\n",
        "\n",
        "$\\vec{a_{h1(t)}} = sigmoid(U\\vec{x} + W\\vec{a_{h1(t-1)}} + \\vec{b_1})$ - aktywacja f. sigmoid jest tylko przykładem,  można użyć innej aktywacji (tanh, relu, etc)\n",
        "\n",
        "$\\vec{a_{o(t)}} = softmax(V\\vec{a_{h1(t)}} + \\vec{b_2})$ - softmax jest przykładem f. aktywacji, ma sens w problemie klasyfikacji przy > 2 etykietach, można również użyć np. sigmoid (jeśli jeden neuron na wyjściu)\n",
        "\n",
        "<br/>\n",
        "\n",
        "gdzie $\\vec{a_{h1(t)}}$ to wartość wektora reprezentującego aktywację warstwy ukrytej w aktualnym kroku,\n",
        "\n",
        "$\\vec{a_{h1(t-1)}}$ to wartość aktywacji warstwy ukrytej w poprzednim kroku,\n",
        "\n",
        "$\\vec{a_{o(t)}}$ to rezultat wygenerowany przez całą sieć neuronową (aktywacja na warstwie \"output\")\n",
        "\n",
        "<br/>\n",
        "Poprzez uwzględnienie wartości warstwy ukrytej z poprzedniego kroku - sieć może nauczyć się w jaki sposób poprzednie dane z sekwencji wpływają na aktualną decyzję.\n",
        "\n",
        "**Zadanie2: Mając zadane wartości macierzy wag: U, V, W a także wartości warstwy ukrytej z poprzedniego kroku oraz wektor cech - zaimplementuj dwie funkcje**:\n",
        "<ol>\n",
        "    <li>get_hidden_state - funkcja obliczająca aktualną wartość wektora w warstwie ukrytej (pamięć o sekwencji).</li>\n",
        "    <li>get_network_output - funkcja obliczająca wyjście sieci dla aktualnie obliczonych wartości w warstwie ukrytej</li>\n",
        "</ol>\n",
        "Dla prostoty późniejszych obliczeń - pomińmy wektor biasów, gdyż nie jest on niezbędny do rozwiązania naszego problemu, wzór na aktywację warstwy ukrytej bez biasów: $\\vec{a_{h1(t)}} = sigmoid(U\\vec{x} + W\\vec{a_{h1(t-1)}})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ft905-0MItnG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.37191738 0.97551727 0.36888573]\n",
            "[0.37811473 0.33866203 0.28322324]\n"
          ]
        }
      ],
      "source": [
        "# policz sigmoidę po wektorze lub skalarze\n",
        "def sigmoid(x): \n",
        "    output = 1 / (1 + np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# policz softmax po wektorze lub skalarze\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "U = np.array([[-3., 2., ], [4., 2., ], [1., -5.,]])\n",
        "W = np.array([[1.25, 1.3, 1.5], [2.01, 3.4, -2.4], [1.08, -.3, 0.1]])\n",
        "V = np.array([[-0.2,0.81, -0.2], [0.12, 0.42, 0.21], [0.1, 0.32, 0.01]])\n",
        "\n",
        "x = np.array([0.5, 0.21])\n",
        "prev_hidden = np.array([0.1, 0.32, 0.01])\n",
        "\n",
        "def get_hidden_state_activation(U, W, x, prev_hidden):\n",
        "    return sigmoid(np.dot(U,x)+np.dot(W,prev_hidden))\n",
        "\n",
        "def get_network_output(V, current_hidden):\n",
        "    return softmax(np.dot(V,current_hidden))\n",
        "\n",
        "\n",
        "hidden_state = get_hidden_state_activation(U, W, x, prev_hidden)\n",
        "print(hidden_state)\n",
        "print(get_network_output(V, hidden_state))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x_cJKI9QItnG"
      },
      "source": [
        "Expected output: \n",
        "\n",
        "0.37191738 0.97551727 0.36888573\n",
        "\n",
        "0.37811473 0.33866203 0.28322324\n",
        "\n",
        "## Zadanie 3 (0.5 pkt)- ilość parametrów\n",
        "\n",
        "Mając zadaną sieć rekurencyjną o jednej warstwie ukrytej, z połączeniem rekurencyjnym w warstwie ukrytej **ile parametrów zostanie optymalizowanych podczas nauki?** Przyjmijmy, że sieć neuronowa ma 20 wejść, warstwa ukryta ma rozmiar 10 neuronów, a wyjściowa - 5 neuronów.\n",
        "\n",
        "a) Nie wliczając biasów (jak w zadaniu 2)\n",
        "\n",
        "b) Uwzględniając biasy na każdym neuronie\n",
        "\n",
        "Odpowiedzi zawrzyj w komentarzach poniżej"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FWwYHWRfItnH"
      },
      "outputs": [],
      "source": [
        "# Odp 3a: 20*10+10*5+10*10 = 200+50+100=350\n",
        "# Odp 3b: 350 + 10 + 5 = 365"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xvfY9SlKItnH"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d7USgf3wJHcj"
      },
      "source": [
        "# Zaawansowane sieci neuronowe w detekcji sentymentu\n",
        "\n",
        "W tej części skupimy się na wykorzystaniu zaawansowanych architektur sieci neuronowych do problemu wykrywania sentymentu (emocji: pozytywnych i negatywnych), które zawarte są w tekstach.\n",
        "\n",
        "Ponieważ implementacja sieci LSTM i GRU jest dość trudna/czasochłonna - wykorzystamy gotowy framework, który pozwoli nam na zdefiniowanie i wyuczenie sieci neuronowej na wysokim poziomie - **Keras**.\n",
        "\n",
        "Ocenę sentymentu przeprowadzimy na gotowym zbiorze recenzji z portalu IMDB, który jest już odpowiednio przeprocesowany i posiada zdefiniowany oczekiwany sentyment dla każdego tekstu (a więc dla którego bez wysiłku możemy uruchomić algorytmy klasyfikacji i je ocenić). Zaczynajmy!\n",
        "\n",
        "## Dane do uczenia\n",
        "\n",
        "Poniższy fragment kodu pobiera dane do uczenia. Funkcja imdb.load_data() ładuje zarówno zbiór uczący (wektory cech, oraz etykiety), jak i analogiczny zbiór testowy. \n",
        "\n",
        "Poniżej wyświetlony na ekranie jest jeden z przykładów uczących oraz przypisana do niego etykieta.\n",
        "\n",
        "Widzimy, że tekst reprezentowany jest sekwencją liczb. Co one oznaczają?\n",
        "Każda liczba reprezentuje słowo (jest identyfikatorem słowa), identyfikatory posortowane są względem częstości występowania słów, zatem słowo o identyfikatorze 10 występuje w korpusie częśćiej niż słowo o identyfikatorze 11.\n",
        "Dodatkowo wprowadzone są specjalne znaczniki BOS - początek zdania i EOS - koniec zdania. Oba równiez reprezentowane są w formie liczbowej.\n",
        "\n",
        "Pamiętamy z jednych z pierwszych laboratoriów, że duża wielkość słownika jest problematyczna. Dobrym pomysłem jest często odrzucenie najrzadziej wystepujących słów, ponieważ one nie mają wielkiego znaczenia (Kiedy uczymy się nowego języka - często nie rozumiemy pojedynczych słów, ale znajomość pozostałych sprawia, że jesteśmy w stanie zrozumieć sens tekstu). Aby ograniczyć rozmiar słownika, w funkcji load_data() możemy zadać parametr num_words o określonej wartości. Wartość ta, mówi nam ile najczęściej występujących słów bierzemy pod uwagę. Wszystkie rzadsze słowa - reprezentowane są zbiorczo taką samą wartością liczbową oznaczającą nieznany token (Unknown token).\n",
        "\n",
        "Inną ważną kwestią jest długość recenzji - każda z nich może składać się z innej liczby słów. O ile sieci rekurencyjne są teoretycznie w stanie poradzić sobie z sekwencjami o różnej długości, to w praktyce optymalizacje wymagają, aby sekwencje były reprezentowane poprzez taką samą długość wektora cech. Aby wyrównać liczbę cech na wejściu stosuje się tzw. padding do określonej długości. Jeśli wektor cech recenzji jest dłuższy niż zadany padding - zostaje on ucięty, jeśli zaś jest krótszy - dodawane są cechy o wartości 0, aby dopełnić długości.\n",
        "\n",
        "**Zadanie 4 (1 punkt)**: Poniższy kod pobiera dane z IMDB ograniczając liczbę słów w słowniku do 10000. \n",
        "Chcielibyśmy przyjrzeć się danym oraz zastosować na nich padding. Aby to zrobić - wykonajmy następujące kroki:\n",
        "<ol>\n",
        "    <li>Sprawdźmy i wyświetlmy średnią długość wektora w x_train - pozwoli nam to sprawdzić ile średnio słów jest w recenzji</li>\n",
        "    <li>Sprawdźmy i wyświetlmy odchylenie standardowe wektora x_train - pozwoli nam to określić jak wygląda rozrzut wartości od średniej</li>\n",
        "    <li>Stosując funkcję pad_sequences z kerasa (zaimportowana w pierwszej linijce) - nadpiszmy zbiory x_train i x_test tak, aby każdy wektor miał długość 500 (https://keras.io/preprocessing/sequence/). Wybrana długość wynika z analizy z poprzednich podpunktów (średnia i odch. std.). Jak teraz wygląda średnia długość i odchylenie std?</li>\n",
        "    <li>Nasz model będziemy weryfikować na zbiorze testowym z użyciem miary accuracy (jaki % podjętych przez klasyfikator decyzji jest poprawnych). Warto sprawdzić jak wygląda rozkład etykiet w zbiorze testowym. Sprawdź: jaki procent zbioru testowego stanowią etykiety o wartości 1? jaki procent zbioru testowego stanowią etykiety o wartości 0?\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "MJvnN6zPItnJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "1\n",
            "Przed paddingiem. Średnia długość wektora: 238.71364; odchylenie std: 176.49367364852034\n",
            "Po paddingu. Średnia długość wektora: 500.0; odchylenie std: 0.0\n",
            "W zbiorze testowym jest 12500 elementów o pozytywnym sentymencie i 12500 elementów o negatywnym. Sentyment pozytywny stanowi 50.0% zbioru.\n"
          ]
        }
      ],
      "source": [
        "# jeśli nie ma kerasa odkomentuj linijkę poniżej\n",
        "#import keras\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.datasets import imdb\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "print(x_train[0]) # pokaż wektor cech dla pierwszej recenzji\n",
        "print(y_train[0]) # pokaż etykietę (1 = sentyment pozytywny; 0 = sentyment negatywny)\n",
        "#print(x_train.size())\n",
        "\n",
        "x_train_sizes=np.array([len(row) for row in x_train])\n",
        "average_x_len =np.mean(x_train_sizes) # todo: oblicz średnią liczbę cech w wektorach x_train (możesz wykorzystać numpy)\n",
        "stddev_x_len = np.std(x_train_sizes) # todo: oblicz odchylenie standardowe po x_train (możesz wykorzystać numpy)\n",
        "\n",
        "\n",
        "x_train = pad_sequences(x_train,500) # TODO: zastosuj padding do 500 tokenów (wskazówka: zobacz na listę importowanych funkcji)\n",
        "x_test = pad_sequences(x_test,500)   # TODO: zastosuj padding do 500 tokenów\n",
        "padded_sizes=np.array([len(row) for row in x_train])\n",
        "\n",
        "padded_average_x_len = np.mean(padded_sizes) # TODO: oblicz średnią liczbę cech w wektorach x_train po paddingu\n",
        "padded_stddev_x_len = np.std(padded_sizes) # TODO: oblicz odchylenie standardowe po x_train po paddingu\n",
        "\n",
        "count_positive = np.count_nonzero(y_test)  # TODO: ile elementów testowych ma przypisany sentyment pozytywny\n",
        "count_negative = y_test.size-count_positive  # TODO: ile elementów testowych ma przypisany sentyment negatywny\n",
        "\n",
        "print(\"Przed paddingiem. Średnia długość wektora: {ave_len}; odchylenie std: {std_dev}\".format(\n",
        "    ave_len=average_x_len, std_dev=stddev_x_len))\n",
        "\n",
        "print(\"Po paddingu. Średnia długość wektora: {ave_len}; odchylenie std: {std_dev}\".format(\n",
        "    ave_len=padded_average_x_len, std_dev=padded_stddev_x_len))\n",
        "\n",
        "print(\"W zbiorze testowym jest {pos} elementów o pozytywnym sentymencie i {neg} elementów o negatywnym. Sentyment pozytywny stanowi {percentage}% zbioru.\".format(\n",
        "pos=count_positive, neg=count_negative, percentage = 100.0*(count_positive)/(count_positive + count_negative)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iTnJJNsQJs2j"
      },
      "source": [
        "## Przykładowa prosta sieć w Keras.\n",
        "Poniżej znajdziecie przykład kodu, którzy tworzy sieć dwuwarstwową o:\n",
        "<ol>\n",
        "<li>100 wejściach</li>\n",
        "<li>warstwie ukrytej z 64 neuronami o aktywacji ReLU</li>\n",
        "<li>warstwie wyjściowej z 1 neuronem o aktywacji sigmoidalnej</li>\n",
        "</ol>\n",
        "Ten kod będzie szablonem dla kolejnych zdań. Uruchom go i sprawdź jak prosta sieć działa \n",
        "\n",
        "$ReLU(x) = max(0, x)$, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IkusQIuCJKba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 87.9539 - accuracy: 0.5041 - val_loss: 5.3137 - val_accuracy: 0.4998\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 2.2442 - accuracy: 0.5132 - val_loss: 1.5936 - val_accuracy: 0.5004\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.9922 - accuracy: 0.5135 - val_loss: 1.1802 - val_accuracy: 0.5003\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7953 - accuracy: 0.5113 - val_loss: 1.0660 - val_accuracy: 0.4979\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7318 - accuracy: 0.5080 - val_loss: 1.0126 - val_accuracy: 0.5020\n",
            "196/196 [==============================] - 0s 1ms/step - loss: 1.0126 - accuracy: 0.5020\n",
            "Trafność klasyfikacji to: 50.199997425079346%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(1337) # for reproducibility\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, LSTM, GRU, Conv1D, MaxPooling1D\n",
        "\n",
        "\n",
        "model = Sequential() # sequential = sieć jako lista warstw, dodajemy warstwy metodą .add() (jak w standardowej liście)\n",
        "model.add(Dense(units=64, input_dim=500, activation='relu')) # dodajemy warstwę Dense (gęstą). Dense oznacza, że wszystkie wejścia (w tym przypadku 100) połączone są z neuronami warstwy w sposób każdy z każdym (każdy neuron z poprzedniej warstwy połączony z każdym neuronem warstwy następnej, tak jak to robiliśmy na poprzednich laboratoriach)\n",
        "model.add(Dense(units=1, activation='sigmoid')) # rozmiar wejścia zdefiniować musimy tylko w pierwszej warstwie (definiujemy ile jest cech na wejściu). Ponieważ model wie jakie są rozmiary poprzednich warstw - może w sposób automatyczny odkryć, że opprzednia warstwa generuje 64 wyjścia\n",
        "\n",
        "model.compile(loss='binary_crossentropy', # budujemy model! ustawiamy funkcję kosztu - mamy klasyfikację z dwiema etykietami, więc stosujemy 'binary_crossentropy'\n",
        "              optimizer='adam',  # wybieramy w jaki sposób sieć ma się uczyć\n",
        "              metrics=['accuracy']) # i wybieramy jaka miara oceny nas interesuje\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test)) # uczymy model na zbiorze treningowym, weryfikujemy na testowym, epochs - oznacza ile przejść po wszystkich przykładachw zbiorze uczącym powinno się wykonać.\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, y_test, batch_size=128) # ostateczna ewaluacja wyuczonego modelu\n",
        "print(\"Trafność klasyfikacji to: {acc}%\".format(acc=accuracy*100)) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "itmuhRxdJ3Uo"
      },
      "source": [
        "Jak widzimy, sieć generuje trafność na poziomie 50%. Ponieważ zarówno etykieta \"1\", jak i \"0\" w zbiorze testowym stanowią połowę - wiemy, że ten klasyfikator nie jest najlepszy (Taką samą trafność będą miały klasyfikatory: zwracające zawsze etykietę 0, zwracajace zawsze etykietę 1 oraz zwracające decyzje losowe).\n",
        "\n",
        "Czy jesteśmy w stanie coś z tym zrobić? \n",
        "Tak. Nasza poprzednia sieć próbowała uczyć się z listy identyfikatorów słów, to stosunkowo kiepska reprezentacja, ale pamiętamy, że całkiem nieźle sprawowały się tzw. Embeddingi. Szczęśliwie - keras udostępnia warstwy uczące się embeddingów z reprezentacji takiej, którą dotychczas podawaliśmy na wejściu.\n",
        "\n",
        "\n",
        "\n",
        "**Zadanie 5 (1 punkt) - Wykorzystanie embeddingów w sieci feed forward**\n",
        "Widząc w jaki sposób dodawane są kolejne warstwy w Kerasie (model.add(...)), przerób architekturę istniejącej sieci w następujący sposób:\n",
        "\n",
        "<ol>\n",
        "    <li>Pierwsza warstwa: Warstwa Embedding (https://keras.io/layers/embeddings/), ustaw długość generowanego wektora na 32, długość wejścia - taka jak wynika to z paddingu - 500, a także rozmiar słownika zgodny z tym co wybraliśmy przy pobieraniu danych (10000)</li>\n",
        "    <li>Druga warstwa: Flatten (https://keras.io/layers/core/); Zauważmy, że warstwa ucząca embeddingi - Embedding - zamienia nam każdy indentyfikator z wektora wejściowego na wektor o zadanej liczbie wymiarów. Każde słowo reprezentowane jest teraz nie pojedynczą liczbą a pojedynczym wektorem. Kiedy złożymy embeddingi wszystkich słów otrzymamy macierz wielkości: liczba słów x rozmiar embeddingu. Warstwa Flatten nie robi nic poza tym, że bierze taką macierz i zamienia znów na wektor poprzez połączenie ze sobą wszystkich wektorów embeddingów w jeden wielki wektor (ustawiając je w jednym wymiarze jeden za drugim) </li>\n",
        "    <li>Trzecia warstwa: klasyczna warstwa Dense (https://keras.io/layers/core/) np. z 64 neuronami i aktywacją relu\n",
        "    <li>Czwarta warstwa (wyjściowa): klasyczna warstwa Dense z 1 neuronem (generującym prawdopodobieństwo pozytywnego sentymentu) i aktywacją sigmoidalną (sigmoid)\n",
        "</ol>\n",
        "Parametry kompilacji, sposób uczenia i ewaluacji możesz pozostawić bez zmian. Czy trafność klasyfikacji wzrosła?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AS8LrqFTJ43W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "196/196 [==============================] - 17s 74ms/step - loss: 0.5868 - accuracy: 0.6336 - val_loss: 0.3273 - val_accuracy: 0.8562\n",
            "Epoch 2/2\n",
            "196/196 [==============================] - 14s 72ms/step - loss: 0.2079 - accuracy: 0.9186 - val_loss: 0.2962 - val_accuracy: 0.8759\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.2962 - accuracy: 0.8759\n",
            "Trafność klasyfikacji to: 87.59199976921082%\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000,output_dim=32,input_length=500))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=64, input_dim=500, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128)\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Trafność klasyfikacji to: {acc}%\".format(acc=accuracy*100)) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sVIHwzsWKCay"
      },
      "source": [
        "## Architektury rekurencyjne\n",
        "\n",
        "Aby zamodelować sieć rekurencyjną LSTM bądź GRU - możemy użyć dedykowanych warstw przygotowanych przez autorów Kerasa.\n",
        "\n",
        "\n",
        "**Zadanie 6 (1 punkt): Sieć rekurencyjna GRU i LSTM**\n",
        "Aby stworzyć taką sieć utwórz model z następującymi warstwami:\n",
        "\n",
        "<ol>\n",
        "    <li>Warstwa Embedding, analogicznie do poprzednich zadań. Rozmiar wektora embeddingów ustawmy na 32</li>\n",
        "    <li>Warstwa LSTM (https://keras.io/layers/recurrent/) - Warstwa sieci rekurencyjnej - nie potrzebuje wcześniejszego spłaszczenia warstwą Flatten. Ustawmy rozmiar tej warstwy na 32. Ponadto ustawmy parametry dropout i recurrent_dropout na 0.2 (parametr regularyzacyjny zabezpieczający przet przeuczeniem)</li>\n",
        "    <li>Warstwa Dense (wyjściowa) - Warstwa o aktywacji sigmoidalnej z 1 neuronem</li>\n",
        "</ol>\n",
        "\n",
        "Po uruchomieniu sieci wykorzystującej LSTM - zamień warstwę LSTM na GRU (https://keras.io/layers/recurrent/) z takimi samymi parametrami - czy sieć uczy się lepiej? Co z czasem uczenia?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8J1x-7j2KII8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "196/196 [==============================] - 144s 676ms/step - loss: 0.5059 - accuracy: 0.7346 - val_loss: 0.3288 - val_accuracy: 0.8634\n",
            "Epoch 2/2\n",
            "196/196 [==============================] - 128s 654ms/step - loss: 0.2756 - accuracy: 0.8903 - val_loss: 0.3057 - val_accuracy: 0.8748\n",
            "782/782 [==============================] - 55s 70ms/step - loss: 0.3057 - accuracy: 0.8748\n",
            "Trafność klasyfikacji to: 87.48000264167786%\n",
            "Czas treningu: 272.3530580997467\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32, input_length=500))\n",
        "model.add(GRU(32, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "start_time = time.time()\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128)\n",
        "end_time = time.time()\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Trafność klasyfikacji to: {acc}%\".format(acc=accuracy*100)) \n",
        "print(\"Czas treningu: {t}\".format(t=end_time - start_time))\n",
        "\n",
        "###LSTM:\n",
        "#Trafność klasyfikacji to: 87.32399940490723%\n",
        "#Czas treningu: 307.7930860519409\n",
        "\n",
        "#Gru:\n",
        "#Trafność klasyfikacji to: 87.48000264167786%\n",
        "#Czas treningu: 272.3530580997467\n",
        "\n",
        "#Gru wypadł zdecydowanie lepiej \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcEJ83aCKMYg"
      },
      "source": [
        "Jak widzimy siec rekurencyjna daje niższe rezultaty niż sieć feedforward. Dlaczego? \n",
        "Sieci rekurencyjne (LSTM i GRU) dają w wielu zadaniach wyniki, które są najlepsze możliwe. Jednakże trening takiej sieci trwa bardzo długo. Gdybyśmy odpowiednio dobrali liczbę warstw i parametry sieci - prawdopodobnie otrzymalibyśmy najlepsze rezultaty ze wsystkich porównywanych architektur - niestety za cenę czasu, który na laboratoriach jest ograniczony.\n",
        "\n",
        "Ponieważ obliczenia w niektórych architekturach są bardzo intensywne, bardzo popularnym jest wykonywanie tych obliczeń nie na procesorze, a na karcie graficznej. W przypadku posiadania dobrej karty graficznej, szybkość przetwarzania będzie dużo większa.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BrMkIJuvKs0T"
      },
      "source": [
        "\n",
        "# Bonus: implementacja sieci rekurencyjnej używając tylko NumPy (niepunktowane)\n",
        "\n",
        "Poniżej znajduje się przykładowa implementacja kodu, który tworzy sieć neuronową, będącą w stanie dodawać do siebie dwie sekwencje binarnych reprezentacji liczb (dwie liczby w formacie binarnym). Tworzona sieć to sieć rekurencyjna, typu many-to-many (synchronizowana)*.\n",
        "\n",
        "(*) Jak wiemy, dodawanie dwóch liczb binarnych może dać na wyjściu liczbę, której reprezentacja binarna będzie dłuższa niż dwie liczby wejściowe, np. \"111 + 001 = 1000\" - nasz problem nie zawiera takich danych - wszystkie dane treningowe dobrane są tak, że długość wyniku nigdy nie będzie dłuższa niż długość wejść. Upraszcza to implementację. Ponadto zaimplementowana sieć nie uwzględnia biasów (nie są wymagane aby rozwiązać problem, a ich pominięcie sprawia, że kod jest bardziej \"kompaktowy\").\n",
        "\n",
        "**Uruchom poniższy kod, sprawdź jak uczy się sieć i przeanalizuj ten fragment kodu. Zadanie nie jest oceniane, ma tylko pokazać jak taka sieć może zostać zaimplementowana**\n",
        "\n",
        "!!! Uwaga: poniższy kod korzysta z Twoich implementacji funkcji **generate_random_matrix** oraz **get_hidden_state_activation** z pierwszej części notebooka. Przed uruchomieniem tego kodu upewnij się, że poprawnie zaimplementował(e/a)ś obie funkcje!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh6PzcJtKc4p"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# policz sigmoidę o wektorze lub skalarze\n",
        "def sigmoid(x): \n",
        "    output = 1 / (1 + np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# policz pochodną z sigmoidy\n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output * (1 - output)\n",
        "\n",
        "# wczytaj dane wejściowe\n",
        "# w formacie liczba1_binarnie,liczba2_binarnie,ich_suma_binarnie\n",
        "# np. 0000111,0000001,0001000\n",
        "def load_dataset(path):\n",
        "    X = []  # w tej liście będziemy zapisywać pary składników\n",
        "    Y = []  # w tej liście będziemy zapisywać sumy składników\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            if len(line.strip()) == 0:\n",
        "                continue\n",
        "            input1_bin, input2_bin, sum_bin = line.strip().split(\",\")\n",
        "            \n",
        "            #wczytane dane to stringi, przekształć na wektory liczb\n",
        "            input1_bin = np.array([int(i) for i in input1_bin])\n",
        "            input2_bin = np.array([int(i) for i in input2_bin])\n",
        "            sum_bin = np.array([int(i) for i in sum_bin])\n",
        "            \n",
        "            X.append((input1_bin, input2_bin)) # zapisz pary składników jako wejścia\n",
        "            Y.append(sum_bin)                  # zapisz sumę jako oczekiwany rezultat\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "train_X, train_Y = load_dataset('dataset.csv')\n",
        "\n",
        "alpha = 0.1      # stała uczenia (learning_rate) - jak duże zmiany robić w uczeniu\n",
        "input_dim = 2    # ile cech (liczb) na wejściu sieci\n",
        "hidden_dim = 16  # rozmiar warstwy ukrytej (rozmiar wektora z pamięcią)\n",
        "output_dim = 1   # ile wartości na wyjściu \n",
        "\n",
        "\n",
        "# initialize neural network weights\n",
        "U = generate_random_matrix(input_dim, hidden_dim)   # inicjalizacja macierzy między wejściem a warstwą ukrytą\n",
        "V = generate_random_matrix(hidden_dim, output_dim)  # inicjalizacja macierzy między warstwą ukrytą a wyjściową\n",
        "W = generate_random_matrix(hidden_dim, hidden_dim)  # inicjalizacja macierzy między poprzednim stanem warstwy ukrytej a aktualnym\n",
        "\n",
        "U_update = np.zeros_like(U) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
        "V_update = np.zeros_like(V) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
        "W_update = np.zeros_like(W) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
        "\n",
        "for j in range(len(train_Y)):    # iterujemy po wszystkich przykładach uczących\n",
        "    added_one_seq = train_X[j][0]   # pierwszy składnik do sumowania w postaci binarnej, np. 00001\n",
        "    added_two_seq = train_X[j][1]   # drugi składnik do sumowania w postaci binarnej, np. 00010\n",
        "    expected_sum_seq = train_Y[j]   # oczekiwany wynik sumowania obu składników (dla przykładu wyżej: 00011)\n",
        "\n",
        "    predicted_sum_seq = np.zeros_like(expected_sum_seq) # tworzymy pusty wektor, który wypełniać będziemy wartościami 0 lub 1 tworząc naszą predykcję\n",
        "\n",
        "    overallError = 0                # tutaj będziemy zapisywać jak bardzo nasze przewidywania różnią się od oczekiwań  \n",
        "\n",
        "    output_l_deltas = list()\n",
        "    hidden_l_values = list() # wartości aktywacji warstwy ukrytej, zapisywane aby\n",
        "    hidden_l_values.append(np.zeros(hidden_dim))\n",
        "\n",
        "    # iterujemy po postaci binarnej bit po bicie, od najmniej znaczącego (od prawej do lewej)\n",
        "    for position in range(len(added_one_seq) - 1, -1, -1):\n",
        "\n",
        "        # jako wejście sieci w aktualnym kroku - bierzemy parę bitów na pozycji [position] (2 liczby)\n",
        "        X = np.array([\n",
        "            [added_one_seq[position], added_two_seq[position]]\n",
        "        ])\n",
        "        # jako oczekiwane wyjście sieci w aktualnym kroku - bierzemy bit na pozycji [position] (budujemy odpowiedź jako sekwencję, znak po znaku!)\n",
        "        y = np.array([[expected_sum_seq[position]]]).T\n",
        "\n",
        "        # Obliczamy wartość warstwy ukrytej\n",
        "        # uwaga, mały hack; w zależności od przygotowania parametrów U, V, W i danych wejściowych (ich orientacji wiersze - kolumny) x\n",
        "        # czasami zdarza się, że odwracamy kolejność operacji na macierzach, tak, aby rozmiary po przekształceniach\n",
        "        # 'pasowały do siebie', mnożenie Ux, może zamienić się na xU. W analizowanym przypadku jest podobnie.\n",
        "        # Dlatego też zamieniłem miejscami kolejność parametrów, podając X za U a hidden_values za W\n",
        "        # w ten sposób odwracamy kolejność mnożenia otrzymując XU + hidden_l_values[-1]V\n",
        "        # więcej informacji pod: https://medium.com/@vivek.yadav/wx-b-vs-xw-b-why-different-formulas-for-deep-neural-networks-in-theory-and-implementation-a5ae6995c4ef\n",
        "        hidden_l = get_hidden_state_activation(X, hidden_l_values[-1], U, W)\n",
        "        \n",
        "        # pobieramy wygenerowane wyjście sieci neuronowej (sigmoida, a nie softmax, gdyż mamy tylko jedno wyjście binarne)\n",
        "        predicted_char = sigmoid(np.dot(hidden_l, V))\n",
        "\n",
        "        # i sprawdzamy jak bardzo nasze przewidywanie się myli w stosunku do oczekiwanego znaku\n",
        "        output_l_error = predicted_char - y\n",
        "\n",
        "        output_l_deltas.append(\n",
        "            (output_l_error) * sigmoid_output_to_derivative(predicted_char)   # loss * grad out\n",
        "        )\n",
        "        overallError += np.abs(output_l_error[0]) # zwiększamy całościowy błąd o błąd z aktualnego kroku, bierzemy wartość bezwzględną, bo znak błędu (nadmiar/niedomiar) nie jest istotny.\n",
        "\n",
        "        predicted_sum_seq[position] = np.round(predicted_char[0][0]) # zapisujemy nasze przewidywanie na odpowiedniej pozycji, zaokrąglając (predicted_char to wartość rzeczywista między 0 a 1, zawiera prawdopodobieństwo tego, że liczba powinna być 1-nką) \n",
        "\n",
        "        hidden_l_values.append(copy.deepcopy(hidden_l)) # zapisz wartość aktywacji warstwy ukrytej, aby można było ją użyć w kolejnym kroku\n",
        "\n",
        "    future_hidden_l_delta = np.zeros(hidden_dim)        \n",
        "    \n",
        "    # propagacja wsteczna \n",
        "    for position in range(len(added_one_seq)):              \n",
        "        X = np.array([\n",
        "            [added_one_seq[position], added_two_seq[position]]\n",
        "        ])  # weź parę liczb od lewej do prawej\n",
        "        hidden_l = hidden_l_values[-position - 1] \n",
        "        prev_hidden_l = hidden_l_values[-position - 2]\n",
        "\n",
        "        # błąd na warstwie wyjściowej\n",
        "        output_l_delta = output_l_deltas[-position - 1]\n",
        "        # błąd na warstwie ukrytej\n",
        "        hidden_l_delta = (future_hidden_l_delta.dot(W.T) + output_l_delta.dot(V.T)) * sigmoid_output_to_derivative(hidden_l)\n",
        "\n",
        "        # zaktualizujmy macierze poprawek względem błędu w aktualnym kroku (na aktualnej pozycji w sekwencji)\n",
        "        V_update += np.atleast_2d(hidden_l).T.dot(output_l_delta)\n",
        "        W_update += np.atleast_2d(prev_hidden_l).T.dot(hidden_l_delta)\n",
        "        U_update += X.T.dot(hidden_l_delta)\n",
        "\n",
        "        future_hidden_l_delta = hidden_l_delta\n",
        "\n",
        "    U -= U_update * alpha  # spadek wag między wejściem a w. ukrytą\n",
        "    V -= V_update * alpha  # spadek wag między w. ukrytą a wyjściem\n",
        "    W -= W_update * alpha  # spadek wag między poprzednią a teraźniejszą w. ukrytą\n",
        "\n",
        "    U_update *= 0  # zeruj macierz jak profesjonalista\n",
        "    V_update *= 0  # zeruj macierz jak profesjonalista\n",
        "    W_update *= 0  # zeruj macierz jak profesjonalista\n",
        "\n",
        "    if(j % 5000 == 0):\n",
        "        print(\"Błąd przykładu:\" + str(overallError))\n",
        "        print(\"Przewidziana sekwencja:\" + str(predicted_sum_seq))\n",
        "        print(\"Oczekiwana sekwencja:  \" + str(expected_sum_seq))\n",
        "        print(\"------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uym5T5qnKvtj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
